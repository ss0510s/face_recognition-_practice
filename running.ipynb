{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8b9c213",
   "metadata": {},
   "source": [
    "# 정확도  측정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc356287",
   "metadata": {},
   "source": [
    "## 개발 환경 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42684305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "773bfa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from library import arcface_onnx\n",
    "from library import model_zoo\n",
    "from library import face_analysis\n",
    "from library import model_func\n",
    "import onnxruntime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a65224",
   "metadata": {},
   "source": [
    "## 모델 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2fd782a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/namsujin/ai_model/face_model\n",
      "input mean and std: 127.5 127.5\n",
      "find model: /Users/namsujin/ai_model/face_model/models/glintr100.onnx recognition\n",
      "find model: /Users/namsujin/ai_model/face_model/models/scrfd_10g_bnkps.onnx detection\n",
      "set det-size: (640, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<library.face_analysis.FaceAnalysis at 0x7fda83801ba8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app = model_func.model_prepare('models')\n",
    "app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8332ba9d",
   "metadata": {},
   "source": [
    "## 모델 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96ec0409",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "진행중 0\n",
      "진행중 1\n",
      "진행중 2\n",
      "진행중 3\n",
      "진행중 4\n",
      "진행중 5\n",
      "진행중 6\n",
      "진행중 7\n",
      "진행중 8\n",
      "진행중 9\n",
      "진행중 10\n",
      "진행중 11\n",
      "진행중 12\n",
      "진행중 13\n",
      "진행중 14\n",
      "진행중 15\n",
      "진행중 16\n",
      "진행중 17\n",
      "진행중 18\n",
      "진행중 19\n",
      "진행중 20\n",
      "진행중 21\n",
      "진행중 22\n",
      "진행중 23\n",
      "진행중 24\n",
      "진행중 25\n",
      "진행중 26\n",
      "진행중 27\n",
      "진행중 28\n",
      "진행중 29\n",
      "진행중 30\n",
      "진행중 31\n",
      "진행중 32\n",
      "진행중 33\n",
      "진행중 34\n",
      "진행중 35\n",
      "진행중 36\n",
      "진행중 37\n",
      "진행중 38\n",
      "진행중 39\n",
      "진행중 40\n",
      "진행중 41\n",
      "진행중 42\n",
      "진행중 43\n",
      "진행중 44\n",
      "진행중 45\n",
      "진행중 46\n",
      "진행중 47\n",
      "진행중 48\n",
      "진행중 49\n",
      "진행중 50\n",
      "진행중 51\n",
      "진행중 52\n",
      "진행중 53\n",
      "진행중 54\n",
      "진행중 55\n",
      "진행중 56\n",
      "진행중 57\n",
      "진행중 58\n",
      "진행중 59\n",
      "진행중 60\n",
      "진행중 61\n",
      "진행중 62\n",
      "진행중 63\n",
      "진행중 64\n",
      "진행중 65\n",
      "진행중 66\n",
      "진행중 67\n",
      "진행중 68\n",
      "진행중 69\n",
      "진행중 70\n",
      "진행중 71\n",
      "진행중 72\n",
      "진행중 73\n",
      "진행중 74\n",
      "진행중 75\n",
      "진행중 76\n",
      "진행중 77\n",
      "진행중 78\n",
      "진행중 79\n",
      "진행중 80\n",
      "진행중 81\n",
      "진행중 82\n",
      "진행중 83\n",
      "진행중 84\n",
      "진행중 85\n",
      "진행중 86\n",
      "진행중 87\n",
      "진행중 88\n",
      "진행중 89\n",
      "진행중 90\n",
      "진행중 91\n",
      "진행중 92\n",
      "진행중 93\n",
      "진행중 94\n",
      "진행중 95\n",
      "진행중 96\n",
      "진행중 97\n",
      "진행중 98\n",
      "진행중 99\n",
      "진행중 100\n",
      "진행중 101\n",
      "진행중 102\n",
      "진행중 103\n",
      "진행중 104\n",
      "진행중 105\n",
      "진행중 106\n",
      "진행중 107\n",
      "진행중 108\n",
      "진행중 109\n",
      "진행중 110\n",
      "진행중 111\n",
      "진행중 112\n",
      "진행중 113\n",
      "진행중 114\n",
      "진행중 115\n",
      "진행중 116\n",
      "진행중 117\n",
      "진행중 118\n",
      "진행중 119\n",
      "진행중 120\n",
      "진행중 121\n",
      "진행중 122\n",
      "진행중 123\n",
      "진행중 124\n",
      "진행중 125\n",
      "진행중 126\n",
      "진행중 127\n",
      "list index out of range\n",
      "진행중 129\n",
      "진행중 130\n",
      "진행중 131\n",
      "진행중 132\n",
      "진행중 133\n",
      "진행중 134\n",
      "진행중 135\n",
      "진행중 136\n",
      "진행중 137\n",
      "진행중 138\n",
      "진행중 139\n",
      "진행중 140\n",
      "진행중 141\n",
      "진행중 142\n",
      "진행중 143\n",
      "진행중 144\n",
      "진행중 145\n",
      "진행중 146\n",
      "진행중 147\n",
      "진행중 148\n",
      "진행중 149\n",
      "진행중 150\n",
      "진행중 151\n",
      "진행중 152\n",
      "진행중 153\n",
      "진행중 154\n",
      "진행중 155\n",
      "진행중 156\n",
      "진행중 157\n",
      "진행중 158\n",
      "진행중 159\n",
      "진행중 160\n",
      "진행중 161\n",
      "진행중 162\n",
      "진행중 163\n",
      "진행중 164\n",
      "진행중 165\n",
      "진행중 166\n",
      "진행중 167\n",
      "진행중 168\n",
      "진행중 169\n",
      "진행중 170\n",
      "list index out of range\n",
      "진행중 172\n",
      "진행중 173\n",
      "진행중 174\n",
      "진행중 175\n",
      "진행중 176\n",
      "진행중 177\n",
      "진행중 178\n",
      "진행중 179\n",
      "진행중 180\n",
      "진행중 181\n",
      "진행중 182\n",
      "진행중 183\n",
      "진행중 184\n",
      "진행중 185\n",
      "진행중 186\n",
      "진행중 187\n",
      "진행중 188\n",
      "진행중 189\n",
      "진행중 190\n",
      "진행중 191\n",
      "진행중 192\n",
      "진행중 193\n",
      "진행중 194\n",
      "진행중 195\n",
      "진행중 196\n",
      "진행중 197\n",
      "진행중 198\n",
      "진행중 199\n"
     ]
    }
   ],
   "source": [
    "is_gray = False\n",
    "predicts = [] \n",
    "# 이미지 파일이 있는 폴더 경로\n",
    "root = '/Users/ss0510s/korean_test_dataset/esrgan_Low/image/'\n",
    "# 1:1 text 파일\n",
    "with open('/Users/ss0510s/korean_test_dataset/LOW/pairs.txt') as f:\n",
    "    pairs_lines = f.readlines()[0:]\n",
    "\n",
    "# 모델 실행\n",
    "with torch.no_grad():\n",
    "    for i in range(200): # pair 개수 만큼 반복\n",
    "        p = pairs_lines[i].replace('\\n', '').split('\\t') # pair을 split하여 리스트로 저장\n",
    "        # 이미지 경로 저장\n",
    "        # 동일 인물\n",
    "        if 3 == len(p):\n",
    "            sameflag = 1\n",
    "            name1 = p[0] + '/' + p[0] + '_' + '{:}.jpg'.format(int(p[1]))\n",
    "            name2 = p[0] + '/' + p[0] + '_' + '{:}.jpg'.format(int(p[2]))\n",
    "        # 동일하지 않은 인물\n",
    "        elif 4 == len(p):\n",
    "            sameflag = 0\n",
    "            name1 = p[0] + '/' + p[0] + '_' + '{:}.jpg'.format(int(p[1]))\n",
    "            name2 = p[2] + '/' + p[2] + '_' + '{:}.jpg'.format(int(p[3]))\n",
    "        else:\n",
    "            raise ValueError(\"WRONG LINE IN 'pairs.txt! \")\n",
    "        # 이미지 파일 open\n",
    "        with open(root + name1, 'rb') as f:\n",
    "            img1 =  Image.open(f)\n",
    "            img1_arr = np.asarray(img1)\n",
    "            im1 = Image.fromarray((img1_arr * 255).astype(np.uint8))\n",
    "            rgb_arr1 = np.asarray(im1.convert('RGB'))  \n",
    "        with open(root + name2, 'rb') as f:\n",
    "            img2 =  Image.open(f)\n",
    "            img2_arr = np.asarray(img2)\n",
    "            im2 = Image.fromarray((img2_arr * 255).astype(np.uint8))\n",
    "            rgb_arr2 = np.asarray(im2.convert('RGB')) \n",
    "        \n",
    "        # 모델 실행\n",
    "        res = app.get(rgb_arr1)\n",
    "        res2 = app.get(rgb_arr2)\n",
    "        \n",
    "        try:\n",
    "            # 임베딩 값 추출\n",
    "            f1 = res[0].embedding\n",
    "            f2 = res2[0].embedding\n",
    "            \n",
    "            print(f'진행중 {i}')\n",
    "            # 두 임베딩값 비교 : 유사할 수록 1에 근접\n",
    "            distance = f1.dot(f2) / (np.linalg.norm(f1) * np.linalg.norm(f2) + 1e-5)\n",
    "        \n",
    "        # detection이 되지 않은 경우\n",
    "        except IndexError as e:\n",
    "            if(sameflag == 1): # 동일 인물인 경우 default 값\n",
    "                distance = 0\n",
    "            elif (sameflag == 0): # 동일 인물이 아닌 경우 default 값\n",
    "                distance = 1\n",
    "            print(e)\n",
    "        \n",
    "        # 두 파일의 이름, distance, sameflag를 리스트에 저장\n",
    "        practice = []\n",
    "        practice.append(name1)\n",
    "        practice.append(name2)\n",
    "        practice.append(distance)\n",
    "        practice.append(sameflag)\n",
    "        predicts.append(practice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1a33f7",
   "metadata": {},
   "source": [
    "## best threshold 추출 함수\n",
    "\n",
    "* thresholds : 임의의 threshold = np.arange(-1.0, 1.0, 0.005)   \n",
    "* predicts : 모델 실행 결과 리스트\n",
    "* t : 임의의 test index를 담은 리스트\n",
    "\n",
    "**predicts에서 index가 t[]에 있는 값만 비교하여 최적의 threshold를 찾아냄**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "371a5518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_threshold(thresholds, predicts, t):\n",
    "    best_threshold = best_acc = 0\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_true = [] # 실제 값\n",
    "        y_predict = [] # 예측 값\n",
    "        \n",
    "        for idx, d in enumerate(predicts):\n",
    "            # index가 t에 있는 값만 비교\n",
    "            if idx in t:\n",
    "                # threshold보다 distance가 크면 동일 인물로 출력\n",
    "                same = 1 if float(d[2]) > threshold else 0\n",
    "                y_predict.append(same) # 예측값 저장\n",
    "                y_true.append(int(d[3])) # 실제값 저장\n",
    "        # 예측값과 실제값 비교\n",
    "        y_true = np.array(y_true)\n",
    "        y_predict = np.array(y_predict)\n",
    "        accuracy = 1.0 * np.count_nonzero(y_true == y_predict) / len(y_true) # 정확도 측정\n",
    "        \n",
    "        # 정확도 측정 후 accuracy가 이전보다 높으면 threshold 업데이트\n",
    "        if accuracy >= best_acc:\n",
    "            best_acc = accuracy\n",
    "            best_threshold = threshold\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa217ee",
   "metadata": {},
   "source": [
    "## 모델 성능 측정 함수\n",
    "\n",
    "* threshold : 기준\n",
    "* diff : 모델 실행 결과가 담긴 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a0fa70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_acc(threshold, diff):\n",
    "    y_true = [] # 정답값\n",
    "    y_predict = [] # 예측값\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for idx, d in enumerate(diff):\n",
    "        # threshold보다 distance가 크면 동일인물로 출력\n",
    "        same = 1 if float(d[2]) > threshold else 0\n",
    "        y_predict.append(same) # 예측값 저장\n",
    "        y_true.append(int(d[3])) # 정답값 저장\n",
    "        \n",
    "        # 값 비교\n",
    "        if(same == 1 and int(d[3])== 1): # 예측값과 실제값이 동일 인물\n",
    "            tp = tp + 1\n",
    "        elif(same == 0 and int(d[3])== 0): # 예측값과 실제값이 다른 인물\n",
    "            tn = tn + 1\n",
    "        elif(same == 1 and int(d[3])== 0): # 예측값은 동일인물, 실제는 다른 인물\n",
    "            fp = fp + 1\n",
    "        if(same == 0 and int(d[3])== 1): # 예측값은 다른 인물, 실제는 동일 인물\n",
    "            fn = fn + 1\n",
    "    \n",
    "    # numpy로 변환\n",
    "    y_true = np.array(y_true)\n",
    "    y_predict = np.array(y_predict)\n",
    "\n",
    "    # recall, precision 계산\n",
    "    rec = float(tp / (tp + fn))\n",
    "    pre = float( tp / (tp + fp ))\n",
    "    \n",
    "    # accuracy 계산\n",
    "    accuracy = 1.0 * np.count_nonzero(y_true == y_predict) / len(y_true)\n",
    "    return rec, pre, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173638b9",
   "metadata": {},
   "source": [
    "## 모델 성능 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa6968f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기화\n",
    "accuracy = 0\n",
    "thd = 0\n",
    "thresholds = np.arange(-1.0, 1.0, 0.005)\n",
    "precision = 0\n",
    "recall = 0\n",
    "indices = 0\n",
    "t = [1, 2, 3,4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n",
    "    101, 102, 103,104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d054c322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78 0.6902654867256637 0.715\n"
     ]
    }
   ],
   "source": [
    "# 함수 실행\n",
    "best_thresh = find_best_threshold(thresholds, predicts, t)\n",
    "\n",
    "recall, precision, accuracy = eval_acc(best_thresh, predicts)\n",
    "\n",
    "print(recall, precision, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba7aa36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
